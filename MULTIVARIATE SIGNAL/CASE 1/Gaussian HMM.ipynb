{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initial State\n",
    "        self.s_0 = nn.Parameter(torch.tensor([5.525, 5.1, 5.1,5.1], requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        self.T = nn.Parameter(torch.tensor(torch.rand(4,4), requires_grad=True)).requires_grad_(True)\n",
    "        self.mu1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        self.mu2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        self.std1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        self.std2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x,t,pred):\n",
    "\n",
    "        x = F.softmax((x.clone()),dim=0)\n",
    "        l=0.0\n",
    "        std1=torch.exp(self.std1).clip(1e-16,1)\n",
    "        std2=torch.exp(self.std2).clip(1e-16,1)\n",
    "        \n",
    "\n",
    "        x_temp=x.clone()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        dist=MultivariateNormal(self.mu1[0]+self.mu2[0]*0.0, (std1[0])*torch.eye(length).to(device))\n",
    "        \n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "\n",
    "        x_temp[0]=o_1*x[0].clone()\n",
    "\n",
    "        dist=MultivariateNormal(self.mu1[1]+self.mu2[1]*0.0,( std1[1])*torch.eye(length).to(device))\n",
    "\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[1]=o_1.clone()*x[1].clone()\n",
    "\n",
    "        dist=MultivariateNormal(self.mu1[2]+self.mu2[2]*0.0, (std1[2])*torch.eye(length).to(device))\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[2]=o_1*x[2].clone()\n",
    "        \n",
    "        dist=MultivariateNormal(self.mu1[3]+self.mu2[3]*0.0, (std1[3])*torch.eye(length).to(device))\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[3]=o_1*x[3].clone()\n",
    "        \n",
    "#         print(x_temp)\n",
    "        \n",
    "\n",
    "\n",
    "        x_temp=x_temp.clip(1e-16,1)\n",
    "        \n",
    "        x=x_temp.clone()/torch.sum(x_temp.clone())\n",
    "        l+=torch.log(torch.sum(x_temp.clone()))# -0.01*state\n",
    "        \n",
    "        for i in range(1,t): \n",
    "\n",
    "            \n",
    "            x=torch.matmul(x,F.softmax(self.T,dim=1))\n",
    "            x_temp=x.clone()\n",
    "            \n",
    "\n",
    "            x_temp1=x.clone()\n",
    "            dist=MultivariateNormal(self.mu1[0]+self.mu2[0]*i, (std1[0])*torch.eye(length).to(device))\n",
    "        \n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "\n",
    "            x_temp[0]=o_1*x[0].clone()\n",
    "\n",
    "            dist=MultivariateNormal(self.mu1[1]+self.mu2[1]*i, (std1[1])*torch.eye(length).to(device))\n",
    "\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[1]=o_1.clone()*x[1].clone()\n",
    "\n",
    "            dist=MultivariateNormal(self.mu1[2]+self.mu2[2]*i,( std1[2])*torch.eye(length).to(device))\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[2]=o_1*x[2].clone()\n",
    "\n",
    "            dist=MultivariateNormal(self.mu1[3]+self.mu2[3]*i, (std1[3])*torch.eye(length).to(device))\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[3]=o_1*x[3].clone()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "#             print(x_temp)\n",
    "#             print(pred[i])\n",
    "            x_temp=x_temp.clip(1e-16,1)\n",
    "\n",
    "            x=x_temp.clone()/torch.sum(x_temp.clone())\n",
    "#             print(x)\n",
    "#             print(\"-----------------------------\")\n",
    "            l+=torch.log(torch.sum(x_temp.clone()))\n",
    "        return l\n",
    "    \n",
    "    def transition(self,x):\n",
    "        x=torch.matmul(x.to(device),F.softmax(self.T,dim=1))\n",
    "        x_temp=x.clone()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7000, 0.1300, 0.1000, 0.0700],\n",
      "        [0.1000, 0.7300, 0.1000, 0.0700],\n",
      "        [0.2020, 0.2525, 0.4747, 0.0707],\n",
      "        [0.1000, 0.0700, 0.0400, 0.7900]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "T_matrix=torch.rand(3, 3).to(device)\n",
    "T_matrix=torch.tensor([[0.7000, 0.1300, 0.1000, 0.0700],\n",
    "        [0.1000, 0.7300, 0.1000, 0.0700],\n",
    "        [0.2020, 0.2525, 0.4747, 0.0707],\n",
    "        [0.1000, 0.0700, 0.0400, 0.7900]], device=device)\n",
    "\n",
    "# T_matrix[1,1]=8\n",
    "\n",
    "# T_matrix[2,2]=8\n",
    "\n",
    "# T_matrix[0,0]=8\n",
    "\n",
    "T_matrix=T_matrix/torch.sum(T_matrix, 1).reshape(4,1)\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,2.5,3.5,5.5]).to(device)\n",
    "O_matrix_mean1=torch.tensor([0.1,0.2,0.3,0.4]).to(device)\n",
    "O_matrix_std=torch.tensor([1.1,1.2,1.3,1.4]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "def simulate_HMM(T_matrix,O_matrix,x,t):\n",
    "    x=0\n",
    "    \n",
    "    \n",
    "    t=0\n",
    "    o=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "#     o.append(Normal(O_matrix_mean[x],O_matrix_std[x]).sample().to(device))\n",
    "    \n",
    "    K=[2*i/(length) for i in range(1,length+1)]\n",
    "    K=torch.tensor(K).to(device)\n",
    "    o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "    t=1\n",
    "    for i in range(1,1000):\n",
    "#         print(i)\n",
    "        x=(torch.distributions.Categorical(T_matrix[x].to(device)).sample())\n",
    "#         print(x)\n",
    "        \n",
    "        \n",
    "        t+=1\n",
    "        if x==0:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        if x==1:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(3*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        if x==2:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(6*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        if x==3:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(9*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        t1=t\n",
    "        if (t==50) :\n",
    "            break\n",
    "    \n",
    "    \n",
    "\n",
    "    return o,t,t1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\893764229.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\893764229.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\893764229.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(6*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\893764229.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(3*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\893764229.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(9*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([ 2.1640,  0.9936,  2.2386,  4.3022, 10.0460,  6.1868,  1.9354,  1.7265,\n",
       "           1.2807,  0.5347]),\n",
       "  tensor([-2.1268,  2.7581, -0.5753,  4.1398, 10.3012,  5.7452,  2.5150,  1.4627,\n",
       "           0.7123, -0.2646]),\n",
       "  tensor([2.4099, 1.0931, 1.6975, 4.5488, 9.1959, 3.9743, 3.3460, 3.8993, 1.8104,\n",
       "          1.3369]),\n",
       "  tensor([ 2.3058,  2.3927,  2.5375,  5.0896,  9.1525,  5.1324,  4.8050,  2.2888,\n",
       "           0.9048, -1.9250]),\n",
       "  tensor([ 1.2196,  0.5995,  3.6313,  5.3297, 10.9328,  6.6280,  4.1134,  2.3997,\n",
       "           2.4527, -0.2711]),\n",
       "  tensor([ 1.4953,  0.0717,  2.0842,  5.2165, 11.3498,  5.8821,  2.8818,  2.0222,\n",
       "          -1.0287,  0.9165]),\n",
       "  tensor([ 2.2865,  1.0823,  3.0332,  7.4022, 12.6398,  6.9428,  2.4627,  2.3650,\n",
       "           2.0600,  0.7292]),\n",
       "  tensor([ 2.0879,  0.1103, -0.6139,  7.9020, 10.6414,  0.9942,  1.5380,  1.5476,\n",
       "          -2.8609,  1.8867]),\n",
       "  tensor([ 2.0269, -2.8653,  2.2267,  7.4384, 10.7431,  3.0958,  5.6709,  2.0922,\n",
       "          -3.1683,  1.1330]),\n",
       "  tensor([ 1.8769, -0.4431,  3.3354,  4.9110,  8.3560,  4.2704,  2.1667,  1.2839,\n",
       "           2.6320, -0.5238]),\n",
       "  tensor([ 0.0911,  3.4917,  4.0829,  7.3089, 12.0078,  7.3104,  3.5103,  1.6036,\n",
       "           0.2175,  0.3543]),\n",
       "  tensor([ 1.4959,  1.0340,  1.6722,  6.7004,  9.9076,  5.0446,  2.6997,  2.4424,\n",
       "          -0.4195, -0.1056]),\n",
       "  tensor([ 0.3367,  1.5165,  3.6508,  5.2634, 11.3615,  7.9902,  2.0600, -0.0495,\n",
       "           1.3465, -1.2963]),\n",
       "  tensor([ 2.3871, -0.5373,  8.7409, -0.6260, 13.5983,  0.4832,  6.9477, -1.4878,\n",
       "           2.5497,  1.8565]),\n",
       "  tensor([-0.7970,  1.9008,  2.7421,  5.1596,  9.9388,  6.8697,  3.3149,  2.0320,\n",
       "           2.0930,  1.1673]),\n",
       "  tensor([ 2.4689,  3.1193,  3.4160,  5.2483, 11.8461,  7.8551,  3.4637,  2.2374,\n",
       "           2.8119,  1.3193]),\n",
       "  tensor([ 6.9736, -2.6923, -0.2204,  8.9995,  9.2559, -1.1599,  5.2126,  2.8859,\n",
       "          -2.5998,  1.1684]),\n",
       "  tensor([1.9934, 4.3085, 3.8224, 2.7616, 7.9155, 4.2057, 2.3244, 5.6315, 3.0176,\n",
       "          0.6807]),\n",
       "  tensor([5.2654, 6.9155, 4.1110, 2.9661, 7.2871, 3.6241, 3.8050, 4.3800, 3.5777,\n",
       "          0.4060]),\n",
       "  tensor([ 3.1138,  4.3695,  4.7130,  4.4124,  4.5135,  2.9667,  4.1236,  4.4047,\n",
       "           4.1311, -0.4767]),\n",
       "  tensor([ 3.2900, -3.6459,  9.8453, -1.6929, 17.2511, -4.1654, 10.9619, -4.2024,\n",
       "           3.5931,  0.0574]),\n",
       "  tensor([ 1.6827, -3.6064,  9.2603, -2.2302, 17.6053, -2.4788,  9.1255, -3.0778,\n",
       "           1.8137,  0.9859]),\n",
       "  tensor([ 3.7715, -2.2956, 11.9144, -3.3758, 18.3895, -3.0530, 10.1993, -4.4690,\n",
       "           4.3983,  0.5779]),\n",
       "  tensor([ 5.3333, -2.1706,  8.8397, -4.4278, 21.8440, -2.6696, 10.6929, -2.4245,\n",
       "           2.3447, -0.5324]),\n",
       "  tensor([ 4.7880, -4.6192, 10.6221, -5.2446, 18.1727, -4.2907,  9.7405, -5.0358,\n",
       "           4.3025,  0.4090]),\n",
       "  tensor([ 3.8238, -5.2256,  8.9430, -3.5784, 19.2409, -6.5480,  9.5261, -6.8602,\n",
       "           2.9831, -0.3807]),\n",
       "  tensor([5.1484, 5.9787, 2.9168, 1.6270, 3.6632, 3.1995, 4.6432, 5.7078, 2.1931,\n",
       "          0.4568]),\n",
       "  tensor([6.2352, 6.0322, 5.2910, 0.4355, 4.6580, 1.3423, 3.6069, 7.2714, 5.4068,\n",
       "          2.0989]),\n",
       "  tensor([3.3760, 7.4913, 3.1678, 1.4496, 3.9224, 2.1712, 2.2066, 5.8285, 4.8387,\n",
       "          1.6695]),\n",
       "  tensor([ 3.2308,  3.1080,  3.7912,  8.5482, 12.6420,  8.5235,  3.5673,  3.2359,\n",
       "           0.9868,  1.9232]),\n",
       "  tensor([ 0.9333,  3.8534,  4.6996,  8.2445, 11.6519,  8.5035,  6.6131,  4.3100,\n",
       "           1.3353,  0.1628]),\n",
       "  tensor([5.9119, 6.7091, 6.5097, 1.9894, 3.3612, 1.2449, 3.5410, 7.3284, 6.2355,\n",
       "          2.0572]),\n",
       "  tensor([5.6064, 7.9251, 3.6817, 0.7677, 3.8501, 3.5232, 2.8704, 6.9741, 5.9898,\n",
       "          0.1831]),\n",
       "  tensor([10.7641, -4.9593, -4.3331, 16.2162, 10.1099, -2.7983,  8.2448,  8.6175,\n",
       "          -8.5145,  1.4638]),\n",
       "  tensor([ 9.9513, -5.6249, -3.9284, 15.2739, 11.1529, -3.7729,  8.3743,  8.3284,\n",
       "          -8.5597, -0.9385]),\n",
       "  tensor([5.5873, 7.3908, 6.3313, 2.6396, 3.3672, 0.8599, 5.7125, 8.9283, 5.1550,\n",
       "          1.1469]),\n",
       "  tensor([5.6377, 6.9776, 5.1791, 1.9180, 3.2205, 2.0356, 4.3329, 7.4178, 7.2344,\n",
       "          1.1606]),\n",
       "  tensor([6.3206, 7.4599, 3.6879, 1.4511, 2.0495, 2.2968, 4.3130, 8.1471, 6.5493,\n",
       "          0.9879]),\n",
       "  tensor([10.7020, -4.4153, -5.2805, 16.0445, 12.5968, -4.8141,  7.2045,  7.3955,\n",
       "          -9.0019, -0.2650]),\n",
       "  tensor([ 1.7753,  4.1778,  4.8892, 10.6139, 13.4504, 10.0696,  5.3701,  4.1983,\n",
       "           2.8081,  2.7181]),\n",
       "  tensor([ 0.4355,  3.1158,  6.9004, 10.5192, 15.1436, 10.3621,  6.3778,  4.0580,\n",
       "           4.9543,  0.6963]),\n",
       "  tensor([  5.5356,  -9.6306,  15.5267,  -9.0528,  25.3596, -10.5892,  12.4359,\n",
       "           -9.6452,   6.0115,   1.1840]),\n",
       "  tensor([  8.1737,  -8.3687,  17.4496, -10.3353,  24.4341, -10.5217,  15.6389,\n",
       "          -10.4411,   6.6956,   0.1743]),\n",
       "  tensor([ 0.1662,  4.1307,  7.4019,  9.4787, 14.4137, 10.0089,  7.6589,  2.0921,\n",
       "           1.1889,  2.3622]),\n",
       "  tensor([ 12.0756,  -7.5600,  -5.6091,  21.0933,   9.0050,  -7.4126,  10.1245,\n",
       "            9.4928, -12.5277,   1.3647]),\n",
       "  tensor([ 9.0608,  9.5118,  6.0477,  0.4086,  0.3718,  0.6449,  5.6075, 11.1831,\n",
       "           6.4738, -0.5724]),\n",
       "  tensor([ 8.1701, 11.2652,  6.6838, -0.5470, -1.2066, -1.6656,  4.5601, 10.4230,\n",
       "           9.7043,  0.4549]),\n",
       "  tensor([ 3.4730,  3.4216,  4.6187,  9.6396, 15.5193,  9.7740,  6.0228,  3.5937,\n",
       "           1.6922,  1.0962]),\n",
       "  tensor([ 2.0432,  5.0697,  7.4168, 10.2574, 16.5269, 10.6192,  6.2659,  4.2709,\n",
       "           2.3842,  1.7981]),\n",
       "  tensor([  8.3529, -11.5362,  18.1249, -14.1330,  28.2250, -14.0320,  19.3838,\n",
       "          -10.6491,   4.5554,   0.5069])],\n",
       " 50,\n",
       " 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\893764229.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\893764229.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\893764229.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(6*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\893764229.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(3*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\893764229.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+O_matrix_mean1[x]*t*torch.sin(9*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\1165437995.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.T = nn.Parameter(torch.tensor(torch.rand(4,4), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\1165437995.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mu1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\1165437995.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mu2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\1165437995.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15832\\1165437995.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: 745.372659444809 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "\n",
    "start_time = time.time()  \n",
    "for batch_size in [5]:\n",
    "    trial =0 \n",
    "    while trial < 5:\n",
    "        t=50\n",
    "        # # batch_size=20\n",
    "        try: \n",
    "            pred1=[]\n",
    "            for i in range(0,batch_size):\n",
    "                pred1.append(simulate_HMM(T_matrix,O_matrix_mean,s_0,t))\n",
    "            import torch.optim as optim\n",
    "            net = Net()\n",
    "            net.to(device)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=.01)\n",
    "            scheduler =lr_scheduler.LinearLR(optimizer,start_factor=1, end_factor=0.001, total_iters=75)\n",
    "            net.train()\n",
    "    \n",
    "            running_loss = 0.0\n",
    "            o,t,t__1=simulate_HMM(T_matrix,O_matrix_mean,s_0,t)\n",
    "            # dist=distribution_initalize(o)\n",
    "            for epoch in range(100):  # loop over the dataset multiple times\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                # forward + backward + optimize\n",
    "    \n",
    "    \n",
    "    \n",
    "                loss=0.0\n",
    "    \n",
    "                for i in range(0,batch_size):\n",
    "                    optimizer.zero_grad()\n",
    "                    loss=0.0\n",
    "    \n",
    "            #         o,t=simulate_HMM(T_matrix,O_matrix_mean,s_0,t)\n",
    "                    o,t,t_11=pred1[i]\n",
    "                    pred=torch.zeros((1,t), dtype=torch.int32)\n",
    "                    pred=o\n",
    "            #         dist=distribution_update(dist,o)\n",
    "                    loss-=net(net.s_0,t,pred)\n",
    "    \n",
    "                    l1_lambda = 0.001  # adjust the L1 regularization parameter as needed\n",
    "                    l1_reg = torch.tensor(0.).to(device)\n",
    "                    for param in net.parameters():\n",
    "                        l1_reg += torch.norm(param, 1).to(device) \n",
    "              \n",
    "                    loss=loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "                    # print statistics\n",
    "            # print(f'[{trial + 1}] loss: {loss/t}')\n",
    "    \n",
    "            # print('Finished Training')\n",
    "    \n",
    "            torch.save(pred1, \"data_normal\\ \" + str(trial)+\" _ \"+str(batch_size)+ \"_data.pt\")\n",
    "            torch.save(net.state_dict(),  \"Model_normal\\ \" + str(trial)+\" _ \"+str(batch_size)+ \"_model.pth\")\n",
    "            trial+=1\n",
    "        except:\n",
    "            print(f'[{trial + 1}] loss: {loss/t}')\n",
    "            \n",
    "            \n",
    "            pass\n",
    "end_time = time.time()  # Capture end time after the loop completes\n",
    "\n",
    "print(f\"Total execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
