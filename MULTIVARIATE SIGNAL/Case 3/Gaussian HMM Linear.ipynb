{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initial State\n",
    "        self.s_0 = nn.Parameter(torch.tensor([5.525, 5.1, 5.1,5.1], requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        self.T = nn.Parameter(torch.tensor(torch.rand(4,4), requires_grad=True)).requires_grad_(True)\n",
    "        self.mu1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        self.mu2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        self.std1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        self.std2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x,t,pred):\n",
    "\n",
    "        x = F.softmax((x.clone()),dim=0)\n",
    "        l=0.0\n",
    "        std1=torch.exp(self.std1).clip(1e-16,1)\n",
    "        std2=torch.exp(self.std2).clip(1e-16,1)\n",
    "        \n",
    "\n",
    "        x_temp=x.clone()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        dist=MultivariateNormal(self.mu1[0]+self.mu2[0]*0.0, (std1[0])*torch.eye(length).to(device))\n",
    "        \n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "\n",
    "        x_temp[0]=o_1*x[0].clone()\n",
    "\n",
    "        dist=MultivariateNormal(self.mu1[1]+self.mu2[1]*0.0,( std1[1])*torch.eye(length).to(device))\n",
    "\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[1]=o_1.clone()*x[1].clone()\n",
    "\n",
    "        dist=MultivariateNormal(self.mu1[2]+self.mu2[2]*0.0, (std1[2])*torch.eye(length).to(device))\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[2]=o_1*x[2].clone()\n",
    "        \n",
    "        dist=MultivariateNormal(self.mu1[3]+self.mu2[3]*0.0, (std1[3])*torch.eye(length).to(device))\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[3]=o_1*x[3].clone()\n",
    "        \n",
    "#         print(x_temp)\n",
    "        \n",
    "\n",
    "\n",
    "        x_temp=x_temp.clip(1e-16,1)\n",
    "        \n",
    "        x=x_temp.clone()/torch.sum(x_temp.clone())\n",
    "        l+=torch.log(torch.sum(x_temp.clone()))# -0.01*state\n",
    "        \n",
    "        for i in range(1,t): \n",
    "\n",
    "            \n",
    "            x=torch.matmul(x,F.softmax(self.T,dim=1))\n",
    "            x_temp=x.clone()\n",
    "            \n",
    "\n",
    "            x_temp1=x.clone()\n",
    "            dist=MultivariateNormal(self.mu1[0]+self.mu2[0]*i, (std1[0])*torch.eye(length).to(device))\n",
    "        \n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "\n",
    "            x_temp[0]=o_1*x[0].clone()\n",
    "\n",
    "            dist=MultivariateNormal(self.mu1[1]+self.mu2[1]*i, (std1[1])*torch.eye(length).to(device))\n",
    "\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[1]=o_1.clone()*x[1].clone()\n",
    "\n",
    "            dist=MultivariateNormal(self.mu1[2]+self.mu2[2]*i,( std1[2])*torch.eye(length).to(device))\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[2]=o_1*x[2].clone()\n",
    "\n",
    "            dist=MultivariateNormal(self.mu1[3]+self.mu2[3]*i, (std1[3])*torch.eye(length).to(device))\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[3]=o_1*x[3].clone()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "#             print(x_temp)\n",
    "#             print(pred[i])\n",
    "            x_temp=x_temp.clip(1e-16,1)\n",
    "\n",
    "            x=x_temp.clone()/torch.sum(x_temp.clone())\n",
    "#             print(x)\n",
    "#             print(\"-----------------------------\")\n",
    "            l+=torch.log(torch.sum(x_temp.clone()))\n",
    "        return l\n",
    "    \n",
    "    def transition(self,x):\n",
    "        x=torch.matmul(x.to(device),F.softmax(self.T,dim=1))\n",
    "        x_temp=x.clone()\n",
    "        return x\n",
    "    def Observation(self,x):\n",
    "\n",
    "#         o_1= self.emission((x))\n",
    "#         o_1[1]=torch.exp(o_1[1])\n",
    "\n",
    "        return mu\n",
    "    \n",
    "    def simulate(self):\n",
    "         o=[]\n",
    "         x1 = 0\n",
    "         for i in range(0,1000):\n",
    "             print(x1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "             if x1==0:\n",
    "                x=torch.tensor([1.0,0.0,0]).to(device)\n",
    "             if x1==1:\n",
    "                x=torch.tensor([0, 1.0,0.0]).to(device)\n",
    "             if x1==2:\n",
    "                x=torch.tensor([0.0,0.0,1]).to(device)\n",
    "             o_1= self.emission((x)) \n",
    "             o_1=o_1/torch.sum(o_1)\n",
    "\n",
    "             o.append(torch.distributions.Categorical(o_1.to(device)).sample())\n",
    "             if x1==2:\n",
    "                \n",
    "                return o\n",
    "             x = self.transition((x))\n",
    "             x1=(torch.distributions.Categorical(x.to(device)).sample()).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # Initial State\n",
    "#         self.s_0 = nn.Parameter(torch.tensor([1.1, 1.1, 0.1], requires_grad=True))\n",
    "        \n",
    "#         # Transition Layers\n",
    "#         self.t1 = nn.Linear(4, 32)\n",
    "#         self.t2 = nn.Linear(32, 64)\n",
    "#         self.t3 = nn.Linear(3, 3)\n",
    "        \n",
    "#         # Emission Layers\n",
    "#         self.o1 = nn.Linear(4, 32)\n",
    "#         self.o2 = nn.Linear(32, 64)\n",
    "#         self.o3 = nn.Linear(3, 10)\n",
    "\n",
    "#     def forward(self, x,t):\n",
    "#         o = torch.zeros((t,10))\n",
    "#         x = F.softmax((x))\n",
    "#         for i in range(0,t):\n",
    "# #             x = F.relu(self.t1(x))\n",
    "# #             x = F.relu(self.t2(x))\n",
    "#             x = F.softmax(self.t3(x))\n",
    "\n",
    "# #             o_1= F.relu(self.o1(x))\n",
    "# #             o_1 = F.relu(self.o2(o_1))\n",
    "#             o_1 = F.softmax(self.o3(x))\n",
    "#             o[i,:]=o_1\n",
    "#         return x,o\n",
    "    \n",
    "#     def transition(self,x):\n",
    "#         x = F.softmax((x))\n",
    "# #         x = F.relu(self.t1(x))\n",
    "# #         x = F.relu(self.t2(x))\n",
    "#         x = F.softmax(self.t3(x))\n",
    "#         return x\n",
    "#     def Observation(self,x):\n",
    "# #         o_1= F.relu(self.o1(x))\n",
    "# #         o_1 = F.relu(self.o2(o_1))\n",
    "#         o_1 = F.softmax(self.o3(x))\n",
    "#         return o_1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7000, 0.1300, 0.1000, 0.0700],\n",
      "        [0.1000, 0.7300, 0.1000, 0.0700],\n",
      "        [0.2020, 0.2525, 0.4747, 0.0707],\n",
      "        [0.1000, 0.0700, 0.0400, 0.7900]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "T_matrix=torch.rand(3, 3).to(device)\n",
    "T_matrix=torch.tensor([[0.7000, 0.1300, 0.1000, 0.0700],\n",
    "        [0.1000, 0.7300, 0.1000, 0.0700],\n",
    "        [0.2020, 0.2525, 0.4747, 0.0707],\n",
    "        [0.1000, 0.0700, 0.0400, 0.7900]], device=device)\n",
    "\n",
    "# T_matrix[1,1]=8\n",
    "\n",
    "# T_matrix[2,2]=8\n",
    "\n",
    "# T_matrix[0,0]=8\n",
    "\n",
    "T_matrix=T_matrix/torch.sum(T_matrix, 1).reshape(4,1)\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "O_matrix_mean=torch.tensor([1.5,2.5,3.5,5.5]).to(device)\n",
    "O_matrix_mean1=torch.tensor([0.1,0.02,0.03,0.04]).to(device)\n",
    "O_matrix_std=torch.tensor([1.1,1.2,1.3,1.4]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_HMM(T_matrix,O_matrix,x,t):\n",
    "    x=0\n",
    "    \n",
    "    \n",
    "    t=0\n",
    "    o=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "#     o.append(Normal(O_matrix_mean[x],O_matrix_std[x]).sample().to(device))\n",
    "    \n",
    "    K=[2*i/(length) for i in range(1,length+1)]\n",
    "    K=torch.tensor(K).to(device)\n",
    "    o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "    t=1\n",
    "    for i in range(1,1000):\n",
    "#         print(i)\n",
    "        x=(torch.distributions.Categorical(T_matrix[x].to(device)).sample())\n",
    "#         print(x)\n",
    "        \n",
    "        \n",
    "        t+=1\n",
    "        if x==0:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        if x==1:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+torch.exp(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(3*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        if x==2:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/5))*torch.sin(6*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        if x==3:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/10))*torch.sin(9*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        t1=t\n",
    "        if (t==50) :\n",
    "            break\n",
    "    \n",
    "    \n",
    "\n",
    "    return o,t,t1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\2656443216.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\2656443216.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\2656443216.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/10))*torch.sin(9*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\2656443216.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/5))*torch.sin(6*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\2656443216.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+torch.exp(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(3*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([ 1.1352,  2.2335,  2.8940,  4.6938, 11.2609,  6.1313,  3.2869,  2.1452,\n",
       "          -0.6953,  1.6519]),\n",
       "  tensor([ 3.7472,  1.1562,  3.7963,  6.7620, 10.2290,  6.5742,  2.0337,  0.2490,\n",
       "           0.4323,  2.4395]),\n",
       "  tensor([-0.6322,  0.9511,  3.4346,  3.3672, 10.9762,  3.5395,  4.0512, -1.2238,\n",
       "           0.2366,  0.1018]),\n",
       "  tensor([ 0.6384,  1.1853,  3.0366,  4.2066, 11.2059,  5.0518,  3.5234, -1.6548,\n",
       "           2.1721,  3.6226]),\n",
       "  tensor([ 1.2761, -0.8851,  5.0255,  5.8385, 11.8901,  2.0180,  2.8694, -0.5111,\n",
       "           0.4427,  0.5586]),\n",
       "  tensor([ 1.9893,  0.1223,  4.5035,  2.5305, 11.3644,  2.9642,  3.7220, -0.7311,\n",
       "           0.9000,  1.4002]),\n",
       "  tensor([ 2.4415, -0.0237,  3.8082,  2.7056, 11.3498,  3.9054,  3.9737, -0.3131,\n",
       "           1.3004,  2.1114]),\n",
       "  tensor([ 0.4676,  1.9416,  3.5761,  4.4782, 12.9611,  5.9147,  3.7973,  0.4859,\n",
       "          -0.5175,  0.3691]),\n",
       "  tensor([ 2.6857, -0.8660,  3.0596,  2.1322, 10.9037,  2.2847,  4.9404,  1.4079,\n",
       "           0.0666,  1.2297]),\n",
       "  tensor([ 4.9146, -0.4723,  2.1847,  5.3690, 10.8635,  3.4559,  1.6364,  4.0951,\n",
       "          -0.1334,  1.3982]),\n",
       "  tensor([5.1190e+00, 2.1028e-01, 6.8581e-04, 8.6130e+00, 9.8681e+00, 5.2468e+00,\n",
       "          4.5786e+00, 2.7693e+00, 5.8350e-01, 2.0946e+00]),\n",
       "  tensor([ 1.3264,  1.7293,  2.5584,  7.6072, 10.5512,  6.8575,  4.8535,  2.5974,\n",
       "           1.2264,  0.6192]),\n",
       "  tensor([1.2652, 2.7284, 3.1379, 6.0593, 9.6014, 6.2464, 1.0669, 2.7363, 2.2650,\n",
       "          0.1296]),\n",
       "  tensor([ 2.0353,  2.9234,  3.5708,  6.4754, 12.7027,  4.8535,  4.8164,  1.6141,\n",
       "           0.4736,  1.1256]),\n",
       "  tensor([ 2.1364,  2.3312,  3.2054,  6.7439, 10.3191,  8.7380,  3.6605,  2.5612,\n",
       "          -0.2174,  1.3060]),\n",
       "  tensor([ 1.8588,  2.1692,  5.1834,  7.5897, 12.7466,  8.1587,  3.6914,  3.3404,\n",
       "           0.2729,  0.5342]),\n",
       "  tensor([ 2.9942,  1.9665,  3.8377,  6.4839, 10.6442,  8.4162,  5.0051,  5.0820,\n",
       "           0.5212,  0.8962]),\n",
       "  tensor([ 0.3935,  1.7389,  3.3346,  8.0362, 12.6564,  6.4074,  2.5883,  2.3994,\n",
       "           1.5902,  1.3899]),\n",
       "  tensor([ 0.5867,  4.4541,  5.3305,  9.0185, 12.7668,  8.9253,  4.1303,  2.1125,\n",
       "          -0.1406,  1.6852]),\n",
       "  tensor([ 0.7592,  2.9616,  2.7661,  5.8257, 11.2968,  7.5827,  4.9550,  0.9250,\n",
       "          -0.8499, -0.6589]),\n",
       "  tensor([ 0.9325, -0.3381,  4.1406,  5.1114, 10.9051,  4.0383,  4.9594, -0.5577,\n",
       "           1.2953, -0.6688]),\n",
       "  tensor([ 2.2044,  0.4954,  4.7883,  1.6944, 11.4844,  2.3776,  3.1689, -0.8360,\n",
       "           2.9881,  0.8229]),\n",
       "  tensor([-0.5805, -0.1312,  4.5638,  3.2752, 14.8739,  2.5046,  5.7929, -3.2058,\n",
       "           3.3328,  0.2872]),\n",
       "  tensor([0.4179, 3.9553, 1.8128, 5.2532, 7.9172, 4.9586, 3.0732, 4.4824, 0.2257,\n",
       "          0.5813]),\n",
       "  tensor([1.0536, 1.9787, 2.8410, 3.6630, 9.1923, 4.2878, 2.9354, 2.7112, 1.6611,\n",
       "          0.7699]),\n",
       "  tensor([ 2.1402,  3.3785,  1.5893,  4.8988, 10.4706,  3.3240,  3.3425,  1.7182,\n",
       "           2.8372,  1.1097]),\n",
       "  tensor([ 0.3629,  4.5157,  4.3424,  7.7681, 13.4170,  6.8201,  4.5506,  1.9839,\n",
       "           0.2904,  0.1521]),\n",
       "  tensor([ 2.2616,  2.7761,  6.0661,  7.6767, 12.3814,  7.4959,  2.9992,  2.0531,\n",
       "           1.6022,  1.6477]),\n",
       "  tensor([ 0.3477,  3.0298,  5.5302,  8.7603, 12.8629,  9.4743,  4.4271,  4.7473,\n",
       "           0.5618,  0.9302]),\n",
       "  tensor([ 0.8261,  2.0078,  5.0052,  6.3969, 11.2522,  8.7943,  4.9905,  2.4762,\n",
       "           0.0593,  0.4930]),\n",
       "  tensor([ 1.3809,  1.5170,  3.4092,  9.3807, 13.1910,  7.1984,  5.2342,  2.6452,\n",
       "           1.0588, -0.8497]),\n",
       "  tensor([ 1.9487,  3.0550,  6.0081,  9.7735, 14.7109,  9.0110,  5.9720,  3.7426,\n",
       "           2.4766,  1.5573]),\n",
       "  tensor([ 0.8195,  2.3155,  5.1143,  9.6933, 12.9687,  9.4500,  5.4633,  2.8936,\n",
       "          -0.6455, -0.0957]),\n",
       "  tensor([2.5862, 2.5454, 1.8022, 6.5207, 9.7803, 5.2406, 2.1125, 3.6950, 2.0741,\n",
       "          1.4477]),\n",
       "  tensor([ 1.4265,  5.6367,  4.5228,  2.6597,  7.2433,  4.9657,  3.3964,  3.7141,\n",
       "           2.2546, -0.0135]),\n",
       "  tensor([2.2525, 2.2026, 3.1266, 3.2030, 7.3726, 2.6940, 3.9312, 3.2905, 4.0290,\n",
       "          0.5782]),\n",
       "  tensor([2.6561, 3.9166, 2.6640, 6.4046, 9.7181, 3.7390, 3.8678, 1.4068, 3.1560,\n",
       "          0.6072]),\n",
       "  tensor([ 1.0779,  2.4712,  2.6383,  3.1538,  7.8629,  4.4521,  2.7380,  5.9916,\n",
       "           1.4729, -1.2036]),\n",
       "  tensor([ 3.5417,  3.3452,  2.3097,  4.7478,  7.1057,  3.5401,  1.4158,  3.7743,\n",
       "           2.8091, -0.9790]),\n",
       "  tensor([2.2823, 3.2098, 3.1494, 4.9932, 8.3708, 3.4831, 1.9055, 3.6956, 3.1982,\n",
       "          1.9455]),\n",
       "  tensor([ 3.2102,  4.2944,  4.1991,  2.7635,  8.1045,  3.9250,  4.9576,  3.7107,\n",
       "           1.6772, -0.1857]),\n",
       "  tensor([3.4890, 2.5536, 4.5561, 3.8332, 5.8040, 2.3347, 3.1535, 3.1067, 2.0671,\n",
       "          0.6793]),\n",
       "  tensor([ 1.9495,  5.3156,  2.4160,  3.3003,  9.1641,  4.4357,  3.7951,  2.4956,\n",
       "           0.6453, -0.3741]),\n",
       "  tensor([ 1.9390, -4.3371,  7.0279, -1.3131, 13.9263, -2.2621,  6.7534, -1.9764,\n",
       "           4.0507,  0.6020]),\n",
       "  tensor([ 3.7797, -3.3825,  6.2822, -0.1516, 18.7681, -0.0390,  7.5558, -2.2269,\n",
       "           2.3884,  0.8981]),\n",
       "  tensor([ 1.4735,  3.1541,  4.5701,  9.0429, 14.6471, 10.2898,  5.0140,  4.9223,\n",
       "           1.2590,  0.7258]),\n",
       "  tensor([ 2.4613,  4.5976,  5.3150, 12.0527, 14.9454, 10.0045,  6.0138,  1.7290,\n",
       "           1.1613, -0.9154]),\n",
       "  tensor([ 2.6749,  2.5984,  7.8260, 11.5021, 14.9347,  9.9640,  6.8066,  4.0752,\n",
       "           0.5005,  0.4662]),\n",
       "  tensor([ 2.2257,  4.0862,  5.5671,  9.2993, 14.0188, 10.3733,  6.8639,  3.2090,\n",
       "           0.3155,  0.3718]),\n",
       "  tensor([ 1.8408,  3.9855,  7.7254,  8.5852, 16.4576, 10.3983,  6.1948,  5.8045,\n",
       "           2.0900,  1.7734])],\n",
       " 50,\n",
       " 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distribution_initalize(o):\n",
    "#     s=torch.zeros(10)\n",
    "#     for i in range(0,10):\n",
    "#         s[i]=torch.sum(o==i)\n",
    "        \n",
    "#     return s#/torch.sum(s)\n",
    "# def distribution_update(dist,o):\n",
    "\n",
    "#     return dist + distribution_initalize(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\2656443216.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\2656443216.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\2656443216.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/5))*torch.sin(6*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\2656443216.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+torch.exp(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(3*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\2656443216.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/10))*torch.sin(9*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\590064641.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.T = nn.Parameter(torch.tensor(torch.rand(4,4), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\590064641.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mu1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\590064641.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mu2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\590064641.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12768\\590064641.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "for batch_size in [5]:\n",
    "    trial =0 \n",
    "    while trial < 5:\n",
    "        t=50\n",
    "        # batch_size=20\n",
    "        try: \n",
    "            pred1=[]\n",
    "            for i in range(0,batch_size):\n",
    "                pred1.append(simulate_HMM(T_matrix,O_matrix_mean,s_0,t))\n",
    "            import torch.optim as optim\n",
    "            net = Net()\n",
    "            net.to(device)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=.01)\n",
    "            scheduler =lr_scheduler.LinearLR(optimizer,start_factor=1, end_factor=0.001, total_iters=75)\n",
    "            net.train()\n",
    "    \n",
    "            running_loss = 0.0\n",
    "            o,t,t__1=simulate_HMM(T_matrix,O_matrix_mean,s_0,t)\n",
    "            # dist=distribution_initalize(o)\n",
    "            for epoch in range(100):  # loop over the dataset multiple times\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                # forward + backward + optimize\n",
    "    \n",
    "    \n",
    "    \n",
    "                loss=0.0\n",
    "    \n",
    "                for i in range(0,batch_size):\n",
    "                    optimizer.zero_grad()\n",
    "                    loss=0.0\n",
    "    \n",
    "            #         o,t=simulate_HMM(T_matrix,O_matrix_mean,s_0,t)\n",
    "                    o,t,t_11=pred1[i]\n",
    "                    pred=torch.zeros((1,t), dtype=torch.int32)\n",
    "                    pred=o\n",
    "            #         dist=distribution_update(dist,o)\n",
    "                    loss-=net(net.s_0,t,pred)\n",
    "    \n",
    "                    l1_lambda = 0.001  # adjust the L1 regularization parameter as needed\n",
    "                    l1_reg = torch.tensor(0.).to(device)\n",
    "                    for param in net.parameters():\n",
    "                        l1_reg += torch.norm(param, 1).to(device) \n",
    "                # #     print(dist)\n",
    "                #     dist1=dist/torch.sum(dist)\n",
    "                #     dist1=dist1.to(device)\n",
    "                    f=0.0\n",
    "                    # for j in range(0,t):\n",
    "                    #         o_1= net.emission((torch.tensor([1.0,j]).to(device)))\n",
    "                    #         e1=Expectation_diff(o_1)\n",
    "    \n",
    "                    #         o_1=net.emission((torch.tensor([2.0,j]).to(device)))\n",
    "                    #         e2=Expectation_diff(o_1)\n",
    "    \n",
    "                    #         o_1= net.emission((torch.tensor([3.0,j]).to(device)))\n",
    "                    #         e3=Expectation_diff(o_1)\n",
    "    \n",
    "    \n",
    "                    #         o_1= net.emission((torch.tensor([4.0,j]).to(device)))\n",
    "                    #         e4=Expectation_diff(o_1)\n",
    "                    #         f+=abs_diff([e1,e2,e3,e4])\n",
    "                   \n",
    "    \n",
    "    \n",
    "                    #+ l1_reg*l1_lambda\n",
    "                #     print(loss)\n",
    "                    loss=loss#+ l1_reg*l1_lambda-0.001*f #+ 10*(torch.sum(t1*t2)/((torch.sum(t1**2)**0.5)*torch.sum(t2**2)**0.5) +torch.sum(t3*t2)/((torch.sum(t3**2)**0.5)*torch.sum(t2**2)**0.5)+torch.sum(t1*t3)/((torch.sum(t1**2)**0.5)*torch.sum(t3**2)**0.5) )\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "                    # print statistics\n",
    "            # print(f'[{trial + 1}] loss: {loss/t}')\n",
    "    \n",
    "            # print('Finished Training')\n",
    "    \n",
    "            torch.save(pred1, \"data_normal\\ \" + str(trial)+\" _ \"+str(batch_size)+ \"_data.pt\")\n",
    "            torch.save(net.state_dict(),  \"Model_normal\\ \" + str(trial)+\" _ \"+str(batch_size)+ \"_model.pth\")\n",
    "            trial+=1\n",
    "        except:\n",
    "            print(f'[{trial + 1}] loss: {loss/t}')\n",
    "            \n",
    "            \n",
    "            pass\n",
    "end_time = time.time()  # Capture end time after the loop completes\n",
    "\n",
    "print(f\"Total execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
