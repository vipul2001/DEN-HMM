{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initial State\n",
    "        self.s_0 = nn.Parameter(torch.tensor([5.525, 5.1, 5.1,5.1], requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        self.T = nn.Parameter(torch.tensor(torch.rand(4,4), requires_grad=True)).requires_grad_(True)\n",
    "        self.mu1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        self.mu2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        self.mu3 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        self.std1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        self.std2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x,t,pred):\n",
    "\n",
    "        x = F.softmax((x.clone()),dim=0)\n",
    "        l=0.0\n",
    "        std1=torch.exp(self.std1).clip(1e-16,1)\n",
    "        std2=torch.exp(self.std2).clip(1e-16,1)\n",
    "        \n",
    "\n",
    "        x_temp=x.clone()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        dist=MultivariateNormal(self.mu1[0]+self.mu2[0]*((torch.tensor(0.0))), (std1[0])*torch.eye(length).to(device))\n",
    "        \n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "\n",
    "        x_temp[0]=o_1*x[0].clone()\n",
    "\n",
    "        dist=MultivariateNormal(self.mu1[1]+torch.exp(self.mu2[1]*(torch.tensor(0.0))),( std1[1])*torch.eye(length).to(device))\n",
    "\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[1]=o_1.clone()*x[1].clone()\n",
    "\n",
    "        dist=MultivariateNormal(self.mu1[2]+(torch.exp(self.mu2[2]*torch.tensor(0.0))+torch.sin(torch.pi*torch.tensor(t)/5)), (std1[2])*torch.eye(length).to(device))\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[2]=o_1*x[2].clone()\n",
    "        \n",
    "        dist=MultivariateNormal(self.mu1[3]+(torch.exp(self.mu2[3]*torch.tensor(0.0))+torch.sin(torch.pi*torch.tensor(t)/10)), (std1[3])*torch.eye(length).to(device))\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[3]=o_1*x[3].clone()\n",
    "        \n",
    "#         print(x_temp)\n",
    "        \n",
    "\n",
    "\n",
    "        x_temp=x_temp.clip(1e-16,1)\n",
    "        \n",
    "        x=x_temp.clone()/torch.sum(x_temp.clone())\n",
    "        l+=torch.log(torch.sum(x_temp.clone()))# -0.01*state\n",
    "        \n",
    "        for i in range(1,t): \n",
    "\n",
    "            \n",
    "            x=torch.matmul(x,F.softmax(self.T,dim=1))\n",
    "            x_temp=x.clone()\n",
    "            \n",
    "\n",
    "            x_temp1=x.clone()\n",
    "            dist=MultivariateNormal(self.mu1[0]+self.mu2[0]*((torch.tensor(i))), (std1[0])*torch.eye(length).to(device))\n",
    "        \n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "\n",
    "            x_temp[0]=o_1*x[0].clone()\n",
    "\n",
    "            dist=MultivariateNormal(self.mu1[1]+torch.exp(self.mu2[1]*(torch.tensor(i))), (std1[1])*torch.eye(length).to(device))\n",
    "\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[1]=o_1.clone()*x[1].clone()\n",
    "\n",
    "            dist=MultivariateNormal(self.mu1[2]+(torch.exp(self.mu2[2]*torch.tensor(i))+torch.sin(torch.pi*torch.tensor(i)/5)),( std1[2])*torch.eye(length).to(device))\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[2]=o_1*x[2].clone()\n",
    "\n",
    "            dist=MultivariateNormal(self.mu1[3]+(torch.exp(self.mu2[3]*torch.tensor(i))+torch.sin(torch.pi*torch.tensor(i)/10)), (std1[3])*torch.eye(length).to(device))\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[3]=o_1*x[3].clone()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "#             print(x_temp)\n",
    "#             print(pred[i])\n",
    "            x_temp=x_temp.clip(1e-16,1)\n",
    "\n",
    "            x=x_temp.clone()/torch.sum(x_temp.clone())\n",
    "#             print(x)\n",
    "#             print(\"-----------------------------\")\n",
    "            l+=torch.log(torch.sum(x_temp.clone()))\n",
    "        return l\n",
    "    \n",
    "    def transition(self,x):\n",
    "        x=torch.matmul(x.to(device),F.softmax(self.T,dim=1))\n",
    "        x_temp=x.clone()\n",
    "        return x\n",
    "    def Observation(self,x):\n",
    "\n",
    "#         o_1= self.emission((x))\n",
    "#         o_1[1]=torch.exp(o_1[1])\n",
    "\n",
    "        return mu\n",
    "    \n",
    "    def simulate(self):\n",
    "         o=[]\n",
    "         x1 = 0\n",
    "         for i in range(0,1000):\n",
    "             print(x1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "             if x1==0:\n",
    "                x=torch.tensor([1.0,0.0,0]).to(device)\n",
    "             if x1==1:\n",
    "                x=torch.tensor([0, 1.0,0.0]).to(device)\n",
    "             if x1==2:\n",
    "                x=torch.tensor([0.0,0.0,1]).to(device)\n",
    "             o_1= self.emission((x)) \n",
    "             o_1=o_1/torch.sum(o_1)\n",
    "\n",
    "             o.append(torch.distributions.Categorical(o_1.to(device)).sample())\n",
    "             if x1==2:\n",
    "                \n",
    "                return o\n",
    "             x = self.transition((x))\n",
    "             x1=(torch.distributions.Categorical(x.to(device)).sample()).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # Initial State\n",
    "#         self.s_0 = nn.Parameter(torch.tensor([1.1, 1.1, 0.1], requires_grad=True))\n",
    "        \n",
    "#         # Transition Layers\n",
    "#         self.t1 = nn.Linear(4, 32)\n",
    "#         self.t2 = nn.Linear(32, 64)\n",
    "#         self.t3 = nn.Linear(3, 3)\n",
    "        \n",
    "#         # Emission Layers\n",
    "#         self.o1 = nn.Linear(4, 32)\n",
    "#         self.o2 = nn.Linear(32, 64)\n",
    "#         self.o3 = nn.Linear(3, 10)\n",
    "\n",
    "#     def forward(self, x,t):\n",
    "#         o = torch.zeros((t,10))\n",
    "#         x = F.softmax((x))\n",
    "#         for i in range(0,t):\n",
    "# #             x = F.relu(self.t1(x))\n",
    "# #             x = F.relu(self.t2(x))\n",
    "#             x = F.softmax(self.t3(x))\n",
    "\n",
    "# #             o_1= F.relu(self.o1(x))\n",
    "# #             o_1 = F.relu(self.o2(o_1))\n",
    "#             o_1 = F.softmax(self.o3(x))\n",
    "#             o[i,:]=o_1\n",
    "#         return x,o\n",
    "    \n",
    "#     def transition(self,x):\n",
    "#         x = F.softmax((x))\n",
    "# #         x = F.relu(self.t1(x))\n",
    "# #         x = F.relu(self.t2(x))\n",
    "#         x = F.softmax(self.t3(x))\n",
    "#         return x\n",
    "#     def Observation(self,x):\n",
    "# #         o_1= F.relu(self.o1(x))\n",
    "# #         o_1 = F.relu(self.o2(o_1))\n",
    "#         o_1 = F.softmax(self.o3(x))\n",
    "#         return o_1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7000, 0.1300, 0.1000, 0.0700],\n",
      "        [0.1000, 0.7300, 0.1000, 0.0700],\n",
      "        [0.2020, 0.2525, 0.4747, 0.0707],\n",
      "        [0.1000, 0.0700, 0.0400, 0.7900]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "T_matrix=torch.rand(3, 3).to(device)\n",
    "T_matrix=torch.tensor([[0.7000, 0.1300, 0.1000, 0.0700],\n",
    "        [0.1000, 0.7300, 0.1000, 0.0700],\n",
    "        [0.2020, 0.2525, 0.4747, 0.0707],\n",
    "        [0.1000, 0.0700, 0.0400, 0.7900]], device=device)\n",
    "\n",
    "# T_matrix[1,1]=8\n",
    "\n",
    "# T_matrix[2,2]=8\n",
    "\n",
    "# T_matrix[0,0]=8\n",
    "\n",
    "T_matrix=T_matrix/torch.sum(T_matrix, 1).reshape(4,1)\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,2.5,3.5,5.5]).to(device)\n",
    "O_matrix_mean1=torch.tensor([0.1,0.02,0.03,0.04]).to(device)\n",
    "O_matrix_std=torch.tensor([1.1,1.2,1.3,1.4]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_HMM(T_matrix,O_matrix,x,t):\n",
    "    x=0\n",
    "    \n",
    "    \n",
    "    t=0\n",
    "    o=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "#     o.append(Normal(O_matrix_mean[x],O_matrix_std[x]).sample().to(device))\n",
    "    \n",
    "    K=[2*i/(length) for i in range(1,length+1)]\n",
    "    K=torch.tensor(K).to(device)\n",
    "    o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "    t=1\n",
    "    for i in range(1,1000):\n",
    "#         print(i)\n",
    "        x=(torch.distributions.Categorical(T_matrix[x].to(device)).sample())\n",
    "#         print(x)\n",
    "        \n",
    "        \n",
    "        t+=1\n",
    "        if x==0:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        if x==1:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+torch.exp(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(3*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        if x==2:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/5))*torch.sin(6*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        if x==3:\n",
    "            o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/10))*torch.sin(9*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )         \n",
    "        t1=t\n",
    "        if (t==50) :\n",
    "            break\n",
    "    \n",
    "    \n",
    "\n",
    "    return o,t,t1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\2656443216.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\2656443216.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\2656443216.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+torch.exp(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(3*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\2656443216.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/5))*torch.sin(6*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\2656443216.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/10))*torch.sin(9*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([-0.7853,  2.6772,  0.8800,  4.0485, 10.5084,  6.5865,  2.7109,  3.3496,\n",
       "           1.3165,  0.4047]),\n",
       "  tensor([ 0.9297,  2.6258,  3.3605,  4.8696,  9.0453,  4.7457,  3.0336,  2.2328,\n",
       "          -0.2257,  1.3956]),\n",
       "  tensor([0.8201, 1.8345, 2.9836, 4.3770, 9.7964, 5.4587, 2.3720, 0.5308, 2.0487,\n",
       "          0.0370]),\n",
       "  tensor([ 1.6360,  1.8075,  0.5902,  7.9903, 10.3331,  3.4356,  3.0733,  3.5327,\n",
       "          -3.4312,  1.6449]),\n",
       "  tensor([1.7953, 2.9062, 1.8170, 4.0087, 9.7071, 4.4587, 2.2957, 0.9265, 1.8966,\n",
       "          1.2987]),\n",
       "  tensor([ 0.4881,  3.8056,  3.2400,  5.3285,  8.6088,  3.9350,  1.9879,  3.6753,\n",
       "           2.0610, -0.8183]),\n",
       "  tensor([1.2267, 1.7203, 2.8013, 6.8720, 7.3312, 5.8511, 3.1498, 0.6216, 0.7340,\n",
       "          1.4771]),\n",
       "  tensor([1.1922, 2.0087, 1.5023, 4.7791, 9.8243, 4.7974, 3.0445, 2.1659, 2.9497,\n",
       "          0.8581]),\n",
       "  tensor([1.3038, 2.2775, 1.2722, 1.4277, 9.0164, 5.2095, 6.0440, 1.6702, 2.4590,\n",
       "          2.4737]),\n",
       "  tensor([ 0.6088, -1.8622,  3.5863,  4.7286, 12.3414,  4.5332,  4.1401, -1.6152,\n",
       "           1.5199, -0.4786]),\n",
       "  tensor([ 2.5132,  0.3079,  3.3400,  1.8487, 12.0144,  4.2961,  2.8876,  1.0261,\n",
       "           1.7628,  1.2750]),\n",
       "  tensor([-3.2352,  1.3526,  4.3926,  3.8237, 11.9890,  5.8785,  3.6972,  1.3389,\n",
       "           3.7906, -0.0471]),\n",
       "  tensor([ 1.2779, -1.2114,  2.8404,  5.4154, 10.9924,  3.0594,  3.4446, -0.1194,\n",
       "           2.4982, -1.1457]),\n",
       "  tensor([ 0.1062,  1.6658,  4.7920,  4.3113, 14.2807,  4.3563,  2.7718,  0.0443,\n",
       "           1.6205,  0.1017]),\n",
       "  tensor([ 3.0218, -0.3810,  3.7166,  4.9245, 11.3124,  4.2596,  5.5554,  2.7244,\n",
       "           2.7803, -0.7828]),\n",
       "  tensor([-0.4502,  2.0273,  2.7845,  3.8371, 11.4025,  2.7767,  1.3594,  0.5733,\n",
       "           2.1300,  0.0122]),\n",
       "  tensor([-0.6265,  1.2683,  3.4302,  3.5820, 12.1295,  6.1544,  2.0058,  1.5515,\n",
       "           0.8878,  0.8381]),\n",
       "  tensor([ 1.1083,  0.5985,  2.6194,  2.4376, 12.8435,  2.5940,  4.3116, -1.5257,\n",
       "          -0.6243,  0.4206]),\n",
       "  tensor([ 0.6478,  0.6479,  2.1038,  4.2989, 11.8240,  3.9504,  2.1534,  0.7911,\n",
       "           1.4254,  1.4021]),\n",
       "  tensor([ 0.9844, -1.2311,  3.4284,  3.2930, 10.0693,  3.5506,  5.3240, -0.6009,\n",
       "          -1.0655,  1.9053]),\n",
       "  tensor([ 1.2409,  1.0007,  4.9812,  3.2534, 11.7692,  5.1362,  4.0970, -1.2446,\n",
       "           1.6667, -0.8822]),\n",
       "  tensor([ 1.4540,  0.9479,  5.8404,  3.1740, 10.4125,  0.8539,  4.6177, -0.1073,\n",
       "           3.2508, -0.4413]),\n",
       "  tensor([ 0.4738,  0.2171,  6.1588, -0.0686, 12.1189,  5.2125,  4.7621, -1.1405,\n",
       "           0.6092,  0.4576]),\n",
       "  tensor([ 2.1933,  0.2512,  6.7080,  2.5462, 16.2294,  3.3182,  4.0377, -2.7013,\n",
       "           1.5707, -0.2555]),\n",
       "  tensor([ 1.0828,  0.2701,  6.5621,  2.5934, 13.4492, -0.1690,  4.6246, -1.5633,\n",
       "           0.3266, -0.9906]),\n",
       "  tensor([ 0.8497, -1.8770,  5.5167,  1.9724, 10.8391,  2.2302,  4.6600,  0.0630,\n",
       "           2.1929, -0.3130]),\n",
       "  tensor([ 0.5101, -0.4968,  5.7695,  2.0026, 14.2281,  1.6543,  4.8448, -2.2817,\n",
       "           3.7414,  1.7357]),\n",
       "  tensor([ 2.9796, -2.7943,  5.7567,  3.4250, 11.7380,  2.3120,  5.7315, -0.3082,\n",
       "          -0.2617,  0.4008]),\n",
       "  tensor([ 2.8760, -0.5276,  3.9082,  2.8440, 14.2499,  1.8481,  5.4967,  0.4651,\n",
       "           2.8814,  3.3229]),\n",
       "  tensor([ 1.9366, -0.6689,  5.5223,  2.0320, 13.1015,  2.5749,  6.1823,  0.2524,\n",
       "           0.3848,  1.6359]),\n",
       "  tensor([ 3.0340, -0.4580,  5.4965,  2.4150, 12.3563,  3.8569,  5.6498, -1.4527,\n",
       "           3.1804, -1.1807]),\n",
       "  tensor([ 3.1268,  2.6507,  4.3851, 10.2788, 14.2813,  9.2239,  6.6978,  1.0978,\n",
       "           1.5097,  0.5148]),\n",
       "  tensor([ 0.4518,  2.9814,  5.1841,  8.7925, 12.4306,  9.2417,  6.6703,  3.3040,\n",
       "           2.6787,  2.2796]),\n",
       "  tensor([ 1.9847,  4.6031,  6.8405,  7.8878, 12.4074,  9.5260,  3.7483,  3.2312,\n",
       "           1.2471,  0.5612]),\n",
       "  tensor([ 2.3480,  2.7924,  4.0565,  8.3975, 12.9456,  8.8450,  4.6653,  1.0031,\n",
       "           2.0940,  0.6292]),\n",
       "  tensor([ 1.0888,  3.3374,  5.0534, 11.1737, 14.6340,  8.1209,  3.9314,  3.2579,\n",
       "           1.9429,  1.2492]),\n",
       "  tensor([ 1.2134,  0.4383,  2.3902,  1.4478, 12.2030,  3.7519,  4.5479, -2.1814,\n",
       "           2.1577,  1.0883]),\n",
       "  tensor([ 3.4504, -1.5363,  5.9725,  4.3098, 15.8748,  1.6002,  5.7783, -0.4300,\n",
       "           1.7910, -1.7336]),\n",
       "  tensor([ 1.0766,  1.6799,  5.6251,  1.0574, 13.8853,  0.7244,  6.2669, -1.7224,\n",
       "           3.1038,  0.9376]),\n",
       "  tensor([ 2.4956, -1.7591,  4.9923,  1.9269, 15.5555,  0.4935,  6.5839, -2.1915,\n",
       "           3.4095, -0.3536]),\n",
       "  tensor([ 4.3665, -1.2570,  0.8857,  7.6107, 10.7768,  2.0307,  4.2288,  3.9023,\n",
       "          -1.9870,  0.4132]),\n",
       "  tensor([2.9537, 2.2095, 3.6238, 3.4795, 7.9386, 2.8842, 3.2485, 3.0432, 1.2249,\n",
       "          1.1882]),\n",
       "  tensor([ 0.9025,  3.3860,  5.7350, 10.9184, 13.5851,  8.4939,  6.1085,  2.6341,\n",
       "           1.2109,  0.3548]),\n",
       "  tensor([ 2.4380,  6.0263,  6.8130, 11.5663, 15.8108,  9.9151,  7.4718,  4.1470,\n",
       "           1.4420,  0.8732]),\n",
       "  tensor([ 3.1856,  1.6354,  6.0727,  9.0214, 16.0338, 10.3987,  7.2536,  4.1736,\n",
       "           0.5944,  0.1383]),\n",
       "  tensor([-5.8317e-03,  3.1187e+00,  8.1408e+00,  9.7303e+00,  1.5374e+01,\n",
       "           1.0486e+01,  5.2797e+00,  3.6517e+00,  1.9160e+00,  2.5497e+00]),\n",
       "  tensor([ 2.6859,  2.5155, -1.4946,  7.8569,  9.9802,  1.4474,  6.0339,  5.2362,\n",
       "          -0.8078,  1.4378]),\n",
       "  tensor([ 3.1706,  5.2357,  6.3405,  8.6121, 12.6600, 10.0572,  7.4741,  3.0343,\n",
       "           2.9671, -1.7565]),\n",
       "  tensor([ 2.6642,  2.6797,  6.9025,  9.6442, 14.3743,  9.6655,  5.7660,  3.9485,\n",
       "           3.9942,  1.0345]),\n",
       "  tensor([ 3.0487,  2.6377,  5.2751,  9.2932, 15.4696,  9.1639,  8.7138,  4.9831,\n",
       "           2.8266, -0.7096])],\n",
       " 50,\n",
       " 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distribution_initalize(o):\n",
    "#     s=torch.zeros(10)\n",
    "#     for i in range(0,10):\n",
    "#         s[i]=torch.sum(o==i)\n",
    "        \n",
    "#     return s#/torch.sum(s)\n",
    "# def distribution_update(dist,o):\n",
    "\n",
    "#     return dist + distribution_initalize(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\2656443216.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\2656443216.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\2656443216.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/5))*torch.sin(6*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\2656443216.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+torch.exp(O_matrix_mean1[x]*torch.tensor(t))*torch.sin(3*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\2656443216.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o.append(MultivariateNormal(0.1*5/(0.05+(torch.tensor(K)-1)**2)+(torch.exp(O_matrix_mean1[x]*torch.tensor(t))+torch.sin(torch.pi*torch.tensor(t)/10))*torch.sin(9*torch.pi*torch.tensor(K/2)/1),O_matrix_std[x]*torch.eye(length)).sample().to(device)   )\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\3829744263.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.T = nn.Parameter(torch.tensor(torch.rand(4,4), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\3829744263.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mu1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\3829744263.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mu2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\3829744263.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mu3 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\3829744263.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std1 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21776\\3829744263.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std2 = nn.Parameter(torch.tensor(torch.rand(4,length), requires_grad=True)).requires_grad_(True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "for batch_size in [5]:\n",
    "    trial =0 \n",
    "    while trial < 5:\n",
    "        t=50\n",
    "        \n",
    "        try: \n",
    "            pred1=[]\n",
    "            for i in range(0,batch_size):\n",
    "                pred1.append(simulate_HMM(T_matrix,O_matrix_mean,s_0,t))\n",
    "            import torch.optim as optim\n",
    "            net = Net()\n",
    "            net.to(device)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=.01)\n",
    "            scheduler =lr_scheduler.LinearLR(optimizer,start_factor=1, end_factor=0.00001, total_iters=75)\n",
    "            net.train()\n",
    "    \n",
    "            running_loss = 0.0\n",
    "            o,t,t__1=simulate_HMM(T_matrix,O_matrix_mean,s_0,t)\n",
    "            # dist=distribution_initalize(o)\n",
    "            for epoch in range(100):  # loop over the dataset multiple times\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                # forward + backward + optimize\n",
    "    \n",
    "    \n",
    "    \n",
    "                loss=0.0\n",
    "    \n",
    "                for i in range(0,batch_size):\n",
    "                    optimizer.zero_grad()\n",
    "                    loss=0.0\n",
    "    \n",
    "            #         o,t=simulate_HMM(T_matrix,O_matrix_mean,s_0,t)\n",
    "                    o,t,t_11=pred1[i]\n",
    "                    pred=torch.zeros((1,t), dtype=torch.int32)\n",
    "                    pred=o\n",
    "            #         dist=distribution_update(dist,o)\n",
    "                    loss-=net(net.s_0,t,pred)\n",
    "                   \n",
    "    \n",
    "    \n",
    "                    #+ l1_reg*l1_lambda\n",
    "                #     print(loss)\n",
    "                    loss=loss #+ 10*(torch.sum(t1*t2)/((torch.sum(t1**2)**0.5)*torch.sum(t2**2)**0.5) +torch.sum(t3*t2)/((torch.sum(t3**2)**0.5)*torch.sum(t2**2)**0.5)+torch.sum(t1*t3)/((torch.sum(t1**2)**0.5)*torch.sum(t3**2)**0.5) )\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "                    # print statistics\n",
    "            # print(f'[{trial + 1}] loss: {loss/t}')\n",
    "    \n",
    "            # print('Finished Training')\n",
    "    \n",
    "            torch.save(pred1, \"data_normal\\ \"+str(batch_size)+\"_lc_\"+ str(trial)+ \"_data.pt\")\n",
    "            torch.save(net.state_dict(),  \"Model_normal\\ \" +str(batch_size)+\"_lc_\"+ str(trial)+ \"_model.pth\")\n",
    "            trial+=1\n",
    "        except:\n",
    "            print(f'[{trial + 1}] loss: {loss/t}')\n",
    "            \n",
    "            \n",
    "            pass\n",
    "end_time = time.time()  # Capture end time after the loop completes\n",
    "\n",
    "print(f\"Total execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
