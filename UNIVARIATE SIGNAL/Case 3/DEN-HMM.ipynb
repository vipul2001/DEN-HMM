{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initial State\n",
    "        self.s_0 = nn.Parameter(torch.tensor([5.525, 5.1, 5.1,5.1], requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        self.T = nn.Parameter(torch.tensor(torch.rand(4,4), requires_grad=True)).requires_grad_(True)\n",
    "        \n",
    "        \n",
    "        self.emission1=nn.Sequential(\n",
    "                                    nn.Linear(16, 16),\n",
    "                                     nn.LeakyReLU(0.2),\n",
    "#                                      nn.Dropout(p=0.2),\n",
    "                                    nn.Linear(16, 1))\n",
    "        self.emission2=nn.Sequential(\n",
    "                                    nn.Linear(16, 16),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "#                                      nn.Dropout(p=0.2),\n",
    "                                    nn.Linear(16, 1))\n",
    "        \n",
    "        self.emission3=nn.Sequential(nn.Linear(2, 8),\n",
    "                                   nn.LeakyReLU(0.2),\n",
    "#                                      nn.Dropout(p=0.2),\n",
    "                                    nn.Linear(8, 16),\n",
    "                                    nn.LeakyReLU(0.2))\n",
    "        self.emission4=nn.Sequential(nn.Linear(2, 8),\n",
    "                                   nn.LeakyReLU(0.2),\n",
    "#                                      nn.Dropout(p=0.2),\n",
    "                                    nn.Linear(8, 16),\n",
    "                                    nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x,t,pred):\n",
    "\n",
    "        x = F.softmax((x.clone()),dim=0)\n",
    "        l=0.0\n",
    "\n",
    "        x_temp=x.clone()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        o_1= self.emission((torch.tensor([1.0,0.0]).to(device)))\n",
    "# #         o_1[1]=torch.exp(o_1[1])\n",
    "        dist=Normal(o_1[0], o_1[1])\n",
    "        \n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "\n",
    "        x_temp[0]=o_1*x[0].clone()\n",
    "\n",
    "        o_1= self.emission((torch.tensor([2.0,0.0]).to(device)))\n",
    "#         o_1[1]=torch.exp(o_1[1])\n",
    "        dist=Normal(o_1[0], o_1[1])\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[1]=o_1.clone()*x[1].clone()\n",
    "\n",
    "        o_1= self.emission((torch.tensor([3.0,0.0]).to(device)))\n",
    "#         o_1[1]=torch.exp(o_1[1])\n",
    "        dist=Normal(o_1[0], o_1[1])\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[2]=o_1*x[2].clone()\n",
    "        \n",
    "        o_1= self.emission((torch.tensor([4.0,0.0]).to(device)))\n",
    "#         o_1[1]=torch.exp(o_1[1])\n",
    "        dist=Normal(o_1[0], o_1[1])\n",
    "        o_1 = torch.exp(dist.log_prob(pred[0]))\n",
    "        x_temp[3]=o_1*x[3].clone()\n",
    "        \n",
    "#         print(x_temp)\n",
    "        \n",
    "\n",
    "\n",
    "        x_temp=x_temp.clip(1e-6,1)\n",
    "        \n",
    "        x=x_temp.clone()/torch.sum(x_temp.clone())\n",
    "        l+=torch.log(torch.sum(x_temp.clone()))# -0.01*state\n",
    "        \n",
    "        for i in range(1,t): \n",
    "\n",
    "            \n",
    "            x=torch.matmul(x,F.softmax(self.T,dim=1))\n",
    "            x_temp=x.clone()\n",
    "            \n",
    "\n",
    "            x_temp1=x.clone()\n",
    "            o_1= self.emission((torch.tensor([1.0,i]).to(device)))\n",
    "#             o_1[1]=torch.exp(o_1[1])\n",
    "            dist=Normal(o_1[0], o_1[1])\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[0]=o_1*x[0]\n",
    "\n",
    "            o_1=self.emission((torch.tensor([2.0,i]).to(device)))\n",
    "#             o_1[1]=torch.exp(o_1[1])\n",
    "            dist=Normal(o_1[0], o_1[1])\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[1]=o_1.clone()*x[1]\n",
    " \n",
    "            o_1= self.emission((torch.tensor([3.0,i]).to(device)))\n",
    "#             o_1[1]=torch.exp(o_1[1])\n",
    "            dist=Normal(o_1[0], o_1[1])\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[2]=o_1*x[2]\n",
    "\n",
    "            o_1= self.emission((torch.tensor([4.0,i]).to(device)))\n",
    "#             o_1[1]=torch.exp(o_1[1])\n",
    "            dist=Normal(o_1[0], o_1[1])\n",
    "            o_1 = torch.exp(dist.log_prob(pred[i]))\n",
    "            x_temp[3]=o_1*x[3].clone()\n",
    "            \n",
    "            \n",
    "            \n",
    "#             print(x_temp)\n",
    "#             print(pred[i])\n",
    "            x_temp=x_temp.clip(1e-6,1)\n",
    "\n",
    "            x=x_temp.clone()/torch.sum(x_temp.clone())\n",
    "#             print(x)\n",
    "#             print(\"-----------------------------\")\n",
    "            l+=torch.log(torch.sum(x_temp.clone()))\n",
    "        return l\n",
    "    \n",
    "    def transition(self,x):\n",
    "        x=torch.matmul(x.to(device),F.softmax(self.T,dim=1))\n",
    "        x_temp=x.clone()\n",
    "        return x\n",
    "    def Observation(self,x):\n",
    "\n",
    "        o_1= self.emission((x))\n",
    "        o_1[1]=torch.exp(o_1[1])\n",
    "\n",
    "        return o_1\n",
    "    def emission(self,x):\n",
    "        x1= self.emission3((x))\n",
    "        x= self.emission4((x))\n",
    "        \n",
    "        o_1= self.emission1((x))\n",
    "        o_2= self.emission2((x1))\n",
    "        o_2=torch.exp(o_2)\n",
    "\n",
    "        return o_1,o_2\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7000, 0.1300, 0.1000, 0.0700],\n",
      "        [0.1000, 0.7300, 0.1000, 0.0700],\n",
      "        [0.2020, 0.2525, 0.4747, 0.0707],\n",
      "        [0.1000, 0.0700, 0.0400, 0.7900]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "T_matrix=torch.rand(3, 3).to(device)\n",
    "T_matrix=torch.tensor([[0.7000, 0.1300, 0.1000, 0.0700],\n",
    "        [0.1000, 0.7300, 0.1000, 0.0700],\n",
    "        [0.2020, 0.2525, 0.4747, 0.0707],\n",
    "        [0.1000, 0.0700, 0.0400, 0.7900]], device=device)\n",
    "\n",
    "# T_matrix[1,1]=8\n",
    "\n",
    "# T_matrix[2,2]=8\n",
    "\n",
    "# T_matrix[0,0]=8\n",
    "\n",
    "T_matrix=T_matrix/torch.sum(T_matrix, 1).reshape(4,1)\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,2.5,3.5,5.5]).to(device)\n",
    "O_matrix_mean1=torch.tensor([0.1,0.02,0.03,0.04]).to(device)\n",
    "O_matrix_std=torch.tensor([1.1,1.2,1.3,1.4]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_HMM(T_matrix,O_matrix,x,t):\n",
    "    x=0\n",
    "    \n",
    "    \n",
    "    t=0\n",
    "    o=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x]+(O_matrix_mean1[x]*torch.tensor(t)),O_matrix_std[x]).sample().to(device))\n",
    "    t=1\n",
    "    for i in range(1,1000):\n",
    "#         print(i)\n",
    "        x=(torch.distributions.Categorical(T_matrix[x].to(device)).sample())\n",
    "#         print(x)\n",
    "        \n",
    "        \n",
    "        t+=1\n",
    "        if x==0:\n",
    "            o.append(Normal(O_matrix_mean[x]+(O_matrix_mean1[x]*torch.tensor(i)),O_matrix_std[x]).sample().to(device))\n",
    "        if x==1:\n",
    "            o.append(Normal(O_matrix_mean[x]+torch.exp(O_matrix_mean1[x]*torch.tensor(i)),O_matrix_std[x]).sample().to(device))\n",
    "        if x==2:\n",
    "#                 print(x)\n",
    "            o.append(Normal(O_matrix_mean[x]+torch.exp(O_matrix_mean1[x]*torch.tensor(i))+torch.sin(torch.pi*torch.tensor(i)/5),O_matrix_std[x]).sample().to(device))\n",
    "        if x==3:\n",
    "            o.append(Normal(O_matrix_mean[x]+torch.exp(O_matrix_mean1[x]*torch.tensor(i))+torch.sin(torch.pi*torch.tensor(i)/10),O_matrix_std[x]).sample().to(device))\n",
    "        t1=t\n",
    "           \n",
    "        if (t==50) :\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    return o,t,t1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.7412, 0.3638, 4.9690, 6.4506, 5.2028, 2.0542, 4.1758, 3.9717, 1.5313,\n",
       "         0.8867, 4.7759, 6.0235, 3.8780, 6.5408, 3.8921, 3.9268, 3.9079, 1.4104,\n",
       "         4.2620, 7.8937, 3.1240, 2.9964, 4.7481, 3.3664, 4.5002, 4.3918, 2.6616,\n",
       "         3.9153, 3.3612, 4.0070, 4.3664, 5.1337, 4.9473, 5.8221, 6.1458, 5.6765,\n",
       "         6.1171, 4.5809, 5.8496, 7.8438, 3.0200, 5.9959, 4.6692, 5.5863, 8.6105,\n",
       "         7.6276, 7.7058, 7.7252, 6.9807, 8.0166]),\n",
       " 50,\n",
       " 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distribution_initalize(o):\n",
    "#     s=torch.zeros(10)\n",
    "#     for i in range(0,10):\n",
    "#         s[i]=torch.sum(o==i)\n",
    "        \n",
    "#     return s#/torch.sum(s)\n",
    "# def distribution_update(dist,o):\n",
    "\n",
    "#     return dist + distribution_initalize(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10048\\3316651859.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.T = nn.Parameter(torch.tensor(torch.rand(4,4), requires_grad=True)).requires_grad_(True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.0\n",
      "[1] loss: 0.0\n",
      "[1] loss: 0.0\n",
      "[2] loss: 0.0\n",
      "[4] loss: 0.0\n",
      "[4] loss: 0.0\n",
      "[5] loss: 0.0\n",
      "Total execution time: 1130.5169491767883 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "\n",
    "start_time = time.time()  \n",
    "\n",
    "for batch_size in [5]:\n",
    "    trial =0 \n",
    "    while trial < 5:\n",
    "        t=50\n",
    "        # batch_size=20\n",
    "        try: \n",
    "            pred1=[]\n",
    "            for i in range(0,batch_size):\n",
    "                pred1.append(simulate_HMM(T_matrix,O_matrix_mean,s_0,t))\n",
    "            import torch.optim as optim\n",
    "            net = Net()\n",
    "            net.to(device)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=.1)\n",
    "            scheduler =lr_scheduler.LinearLR(optimizer,start_factor=1, end_factor=0.001, total_iters=75)\n",
    "            net.train()\n",
    "    \n",
    "            running_loss = 0.0\n",
    "            o,t,t__1=simulate_HMM(T_matrix,O_matrix_mean,s_0,t)\n",
    "            # dist=distribution_initalize(o)\n",
    "            for epoch in range(100):  # loop over the dataset multiple times\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                # forward + backward + optimize\n",
    "    \n",
    "    \n",
    "    \n",
    "                loss=0.0\n",
    "    \n",
    "                for i in range(0,batch_size):\n",
    "                    optimizer.zero_grad()\n",
    "                    loss=0.0\n",
    "    \n",
    "            #         o,t=simulate_HMM(T_matrix,O_matrix_mean,s_0,t)\n",
    "                    o,t,t_11=pred1[i]\n",
    "                    pred=torch.zeros((1,t), dtype=torch.int32)\n",
    "                    pred=o\n",
    "            #         dist=distribution_update(dist,o)\n",
    "                    loss-=net(net.s_0,t,pred.to(device))\n",
    "    \n",
    "                    l1_lambda = 0.001  # adjust the L1 regularization parameter as needed\n",
    "                    l1_reg = torch.tensor(0.).to(device)\n",
    "                    for param in net.parameters():\n",
    "                        l1_reg += torch.norm(param, 1).to(device) \n",
    "                # #     print(dist)\n",
    "                #     dist1=dist/torch.sum(dist)\n",
    "                #     dist1=dist1.to(device)\n",
    "                    f=0.0\n",
    "                    for j in range(0,t):\n",
    "                            o_1= net.emission((torch.tensor([1.0,j]).to(device)))\n",
    "                            e1=Expectation_diff(o_1)\n",
    "    \n",
    "                            o_1=net.emission((torch.tensor([2.0,j]).to(device)))\n",
    "                            e2=Expectation_diff(o_1)\n",
    "    \n",
    "                            o_1= net.emission((torch.tensor([3.0,j]).to(device)))\n",
    "                            e3=Expectation_diff(o_1)\n",
    "    \n",
    "    \n",
    "                            o_1= net.emission((torch.tensor([4.0,j]).to(device)))\n",
    "                            e4=Expectation_diff(o_1)\n",
    "                            f+=abs_diff([e1,e2,e3,e4])\n",
    "                   \n",
    "    \n",
    "    \n",
    "                    #+ l1_reg*l1_lambda\n",
    "                #     print(loss)\n",
    "                    loss=loss+ l1_reg*l1_lambda-0.001*f #+ 10*(torch.sum(t1*t2)/((torch.sum(t1**2)**0.5)*torch.sum(t2**2)**0.5) +torch.sum(t3*t2)/((torch.sum(t3**2)**0.5)*torch.sum(t2**2)**0.5)+torch.sum(t1*t3)/((torch.sum(t1**2)**0.5)*torch.sum(t3**2)**0.5) )\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "                    # print statistics\n",
    "            # print(f'[{trial + 1}] loss: {loss/t}')\n",
    "    \n",
    "            # print('Finished Training')\n",
    "    \n",
    "            torch.save(pred1, \"data_deep\\ \" + str(trial)+\" _ \"+str(batch_size)+ \"_data.pt\")\n",
    "            torch.save(net.state_dict(),  \"Model_deep\\ \" + str(trial)+\" _ \"+str(batch_size)+ \"_model.pth\")\n",
    "            trial+=1\n",
    "        except:\n",
    "            print(f'[{trial + 1}] loss: {loss/t}')\n",
    "            \n",
    "            \n",
    "            pass\n",
    "end_time = time.time()  # Capture end time after the loop completes\n",
    "\n",
    "print(f\"Total execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
